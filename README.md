# Local LLM

This repository hosts a docker-compose stack for locally running LLM with Ollama
and Open WebUI.
